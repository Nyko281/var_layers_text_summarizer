{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textstat\n",
    "import torch\n",
    "\n",
    "from evaluate import load\n",
    "from nltk.translate import meteor_score\n",
    "from nltk import word_tokenize\n",
    "from torchmetrics.text.rouge import ROUGEScore\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_avbl, device = test_cuda_avbl()\n",
    "\n",
    "MODEL           = 'gpt2' #{gpt2, gpt2-medium, gpt2-large, gpt2-xl}                                                      # set model\n",
    "\n",
    "SPECIAL_TOKENS  = {\"bos_token\": \"<|BOS|>\",                                                                              # beginning of a sequenze\n",
    "                   \"eos_token\": \"<|EOS|>\",                                                                              # end of a sequenze\n",
    "                   \"unk_token\": \"<|UNK|>\",                                                                              # set for unknown tokens\n",
    "                   \"pad_token\": \"<|PAD|>\",                                                                              # empty tokens for short sentences\n",
    "                   \"sep_token\": \"<|SEP|>\"}                                                                              # seperates sentences\n",
    "\n",
    "MAX_LENGTH      = 1024\n",
    "\n",
    "TRAIN_SIZE      = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_testing = pd.read_excel(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(MODEL, special_tokens=SPECIAL_TOKENS)\n",
    "model = get_model(MODEL, cuda_avbl, tokenizer, special_tokens=SPECIAL_TOKENS,\n",
    "                load_model_path=r\"model\\pytorch_model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Summarizer():\n",
    "    '''\n",
    "    This is a class for summarizing abstracts of scientifict paper.\n",
    "\n",
    "    Attributes:\n",
    "        model (model class): NLP model for summarization.\n",
    "        tokenizer (tokenizer class): tokenizer for preparing inputs for model.\n",
    "        cuda_avbl (bool): Is Cuda for data processing available?\n",
    "    '''\n",
    "\n",
    "    def __init__(self, model, tokenizer, cuda_avbl):\n",
    "        '''\n",
    "        Constructor for Summarizer class.\n",
    "        \n",
    "        Parameters:\n",
    "            model (model class): NLP model for summarization.\n",
    "            tokenizer (tokenizer class): tokenizer for preparing inputs for model.\n",
    "            cuda_avbl (bool): Is Cuda for data processing available?\n",
    "        '''\n",
    "\n",
    "        self.model = model \n",
    "        self.tokenizer = tokenizer\n",
    "        self.cuda_avbl = cuda_avbl\n",
    "        logging.info(\"Summarizer instantiated\")\n",
    "\n",
    "\n",
    "    def __prep_text(self):\n",
    "        '''Prepares inputs for NLP model.'''\n",
    "\n",
    "        self.in_text = self.in_text.strip().replace(\"\\n\", \" \")\n",
    "\n",
    "        prompt = \"<|BOS|>\" + self.in_text + \"TL;DR:\"\n",
    "        tokenized_prompt = self.tokenizer.encode(prompt)\n",
    "        if len(tokenized_prompt) > MAX_LENGTH:\n",
    "            trunc = int((MAX_LENGTH-self.max_len)/2)\n",
    "            tokenized_prompt = tokenized_prompt[:trunc] + tokenized_prompt[-trunc:]\n",
    "        self.encoded_in = torch.tensor(tokenized_prompt).unsqueeze(0)\n",
    "        self.in_ids_len = len(tokenized_prompt)\n",
    "        logging.info(\"Model inputs prepared\")\n",
    "        \n",
    "        if self.cuda_avbl:\n",
    "            device = torch.device(\"cuda\")\n",
    "            self.encoded_in = self.encoded_in.to(device)\n",
    "    \n",
    "\n",
    "    def get_summary(self, in_text, max_len, min_len):\n",
    "        '''\n",
    "        Generates summaries with NLP model.\n",
    "\n",
    "        Parameters:\n",
    "            in_text (str): Input text to summarize.\n",
    "            max_len (int): Maximum length of generated summary.\n",
    "            min_len (int): Minimum length of generated summary.\n",
    "\n",
    "        Returns:\n",
    "            output (str): The generated summary from data.\n",
    "        '''\n",
    "\n",
    "        self.in_text = in_text\n",
    "        self.max_len = max_len\n",
    "        self.__prep_text()\n",
    "        self.model.eval()\n",
    "\n",
    "        sample_outputs = self.model.generate(inputs=self.encoded_in,    \n",
    "                                max_length=self.in_ids_len+max_len,                                                 # max lenght of generated text \n",
    "                                min_length=self.in_ids_len+min_len,                                                 # min lenght of generated text\n",
    "                                do_sample=True,                                                                     # sampling or always using word with highest probability\n",
    "                                early_stopping=True,                                                                # stopping beamch search when num_beam sentences finished        \n",
    "                                temperature=0.5,                                                                    # scales probabilities for a more conservative (lower) or divers (higher) model\n",
    "                                top_k=30,                                                                           # number of most propable tokens to keep                                \n",
    "                                top_p=0.7,                                                                          # keeping only most propable tokens for generation \n",
    "                                repetition_penalty=2.0,                                                             # avoiding sentences that repeat themselves\n",
    "                                num_return_sequences=1                                                              # number of returned descriptions\n",
    "                                )\n",
    "        \n",
    "        logging.info(\"Summary generated\")\n",
    "\n",
    "        for i, sample_output in enumerate(sample_outputs):\n",
    "            self.smry = self.tokenizer.decode(sample_output, skip_special_tokens=True)\n",
    "            output = self.smry.split(\"TL;DR:\",1)[1]\n",
    "\n",
    "        return output.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_rouge_score(original, generate):\n",
    "    '''\n",
    "    Calculates Rouge Score for a generated summary and a reference text.\n",
    "\n",
    "    Parameters:\n",
    "        original (str): Reference summary.\n",
    "        generate (str): NLP-Model generated summary.\n",
    "\n",
    "    Returns:\n",
    "        list: Multiple Rouge Scores.\n",
    "    '''\n",
    "\n",
    "    rouge = ROUGEScore(use_stemmer=True, accumulate=\"best\", rouge_keys=(\"rouge1\", \"rougeLsum\"))\n",
    "\n",
    "    rouge_score      = rouge(generate, original)\n",
    "    rouge1_fmeasure  = round(rouge_score[\"rouge1_fmeasure\"].item(), 2)\n",
    "    rouge1_precision = round(rouge_score[\"rouge1_precision\"].item(), 2)\n",
    "    rouge1_recall    = round(rouge_score[\"rouge1_recall\"].item(), 2)\n",
    "    rougeLsum_fmeasure  = round(rouge_score[\"rougeLsum_fmeasure\"].item(), 2)\n",
    "    rougeLsum_precision = round(rouge_score[\"rougeLsum_precision\"].item(), 2)\n",
    "    rougeLsum_recall    = round(rouge_score[\"rougeLsum_recall\"].item(), 2)\n",
    "    \n",
    "    return [rouge1_fmeasure, rouge1_precision, rouge1_recall, rougeLsum_fmeasure, rougeLsum_precision, rougeLsum_recall]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_bert_score(original, generate):\n",
    "    '''\n",
    "    Calculates Bert Score for a generated summary and a reference text.\n",
    "\n",
    "    Parameters:\n",
    "        original (str): Reference summary.\n",
    "        generate (str): NLP-Model generated summary.\n",
    "\n",
    "    Returns:\n",
    "        list: Multiple Bert Scores.\n",
    "    '''\n",
    "\n",
    "    bertscore = load(\"bertscore\")\n",
    "\n",
    "    bert_results = bertscore.compute(predictions=[generate], references=[original], lang=\"en\")\n",
    "    bert_precision = round(bert_results[\"precision\"][0], 2)\n",
    "    bert_recall = round(bert_results[\"recall\"][0], 2)\n",
    "    bert_f1 = round(bert_results[\"f1\"][0], 2)\n",
    "\n",
    "    return [bert_f1, bert_precision, bert_recall]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_meteor_score(original, generate):\n",
    "    '''\n",
    "    Calculates Meteor Score for a generated summary and a reference text.\n",
    "\n",
    "    Parameters:\n",
    "        original (str): Reference summary.\n",
    "        generate (str): NLP-Model generated summary.\n",
    "\n",
    "    Returns:\n",
    "        list: Meteor Score.\n",
    "    '''\n",
    "\n",
    "    meteorscore = meteor_score.single_meteor_score(\n",
    "        word_tokenize(original),\n",
    "        word_tokenize(generate))\n",
    "\n",
    "    return [round(meteorscore, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_flesch_score(generate):\n",
    "    '''\n",
    "    Calculates Flesch reading ease for a generated text.\n",
    "\n",
    "    Parameters:\n",
    "        generate (str): NLP-Model generated summary.\n",
    "\n",
    "    Returns:\n",
    "        list: Meteor Score.\n",
    "    '''\n",
    "\n",
    "    textstat.set_lang(\"en\")\n",
    "    flesch_index = textstat.flesch_reading_ease(generate)\n",
    "\n",
    "    return [round(flesch_index, 2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(original, generate):\n",
    "    '''\n",
    "    Gets scores from multiple functions and summarizes all values ​​in one list.\n",
    "\n",
    "    Parameters:\n",
    "        original (str): Reference summary.\n",
    "        generate (str): NLP-Model generated summary.\n",
    "\n",
    "    Returns:\n",
    "        scores (list): Multiple Scores from different NLP text metrics.\n",
    "    '''\n",
    "\n",
    "    rouge_scores  = calc_rouge_score(original, generate)\n",
    "    bert_scores   = calc_bert_score(original, generate)\n",
    "    meteor_scores = calc_meteor_score(original, generate)\n",
    "    flesch_scores = calc_flesch_score(generate)\n",
    "\n",
    "    scores = [*rouge_scores, *bert_scores, *meteor_scores, *flesch_scores]\n",
    "    \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame(columns=[\"rouge1_fmeasure\", \"rouge1_precision\", \"rouge1_recall\", \"rougeLsum_fmeasure\", \"rougeLsum_precision\", \"rougeLsum_recall\", \"bert_f1\", \"bert_precision\", \"bert_recall\", \"meteor\", \"flesch\"])\n",
    "gpt_summarizer = Summarizer(model, tokenizer, cuda_avbl)\n",
    "        \n",
    "for index, row in df_testing.iterrows():\n",
    "\n",
    "    #target = row[\"Abstract\"]\n",
    "    target = row[\"One Sentence\"]\n",
    "    print(f\"Target:\\n{target}\")\n",
    "\n",
    "    #generated = gpt_summarizer.get_summary(row[\"Title\"] + row[\"Full Text\"], max_len=200, min_len=100)\n",
    "    #generated = gpt_summarizer.get_summary(row[\"Title\"] + row[\"Abstract\"], max_len=50, min_len=20)\n",
    "    generated = gpt_summarizer.get_summary(row[\"Title\"] + row[\"Full Text\"], max_len=50, min_len=20)  \n",
    "    print(f\"Generated:\\n{generated}\")\n",
    "\n",
    "    new_row = get_scores(target, generated) \n",
    "    df_results.loc[len(df_results)] = new_row\n",
    "    \n",
    "    print(f\"{new_row}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results[[\"rouge1_precision\", \"rouge1_recall\", \"rouge1_fmeasure\", \"bert_precision\", \"bert_recall\", \"bert_f1\"]].mean()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
