{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch \n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "from functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_avbl, device = test_cuda_avbl()\n",
    "\n",
    "MODEL           = 'gpt2' #{gpt2, gpt2-medium, gpt2-large, gpt2-xl}                                                      # set model\n",
    "\n",
    "SPECIAL_TOKENS  = {\"bos_token\": \"<|BOS|>\",                                                                              # beginning of a sequenze\n",
    "                   \"eos_token\": \"<|EOS|>\",                                                                              # end of a sequenze\n",
    "                   \"unk_token\": \"<|UNK|>\",                                                                              # set for unknown tokens\n",
    "                   \"pad_token\": \"<|PAD|>\",                                                                              # empty tokens for short sentences\n",
    "                   \"sep_token\": \"<|SEP|>\"}                                                                              # seperates sentences\n",
    "\n",
    "MAX_LENGTH      = 1024\n",
    "\n",
    "TRAIN_SIZE      = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicData():\n",
    "    '''\n",
    "    This is a class for loading and extracting the basic data.\n",
    "\n",
    "    Attributes:\n",
    "        data_path (str): Path of the json trainings data.\n",
    "        read_lines (bool): Should the file be read as a lined object?\n",
    "        join_lists (bool):  Is it necessary to join multipile strings to one source?\n",
    "        source_min_max (list): Minimum and maximum string length of the source.\n",
    "        target_min_max (list): Minimum and maximum string length of the target.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, data_path, read_lines=False, join_lists=False, source_min_max=[0, 100000], target_min_max=[20, 250]):\n",
    "        '''\n",
    "        Constructor for BasicData class.\n",
    "        \n",
    "        Parameters:\n",
    "            data_path (str): Path of the json trainings data.\n",
    "            read_lines (bool): Should the file be read as a lined object?\n",
    "            join_lists (bool):  Is it necessary to join multipile strings to one source?\n",
    "            source_min_max (list): Minimum and maximum string length of the source.\n",
    "            target_min_max (list): Minimum and maximum string length of the target.\n",
    "        '''\n",
    "\n",
    "        self.data_path      = data_path\n",
    "        self.read_lines     = read_lines\n",
    "        self.join_lists     = join_lists\n",
    "        self.source_min_max = source_min_max\n",
    "        self.target_min_max = target_min_max\n",
    "\n",
    "        self.df  = self.get_data()\n",
    "        self.dic = self.get_dict()\n",
    "\n",
    "        logging.info(\"BasicData instantiated\")\n",
    "    \n",
    "\n",
    "    def get_data(self):\n",
    "        '''\n",
    "        Reads json file and converts it to pandas data frame.\n",
    "        \n",
    "        Returns:\n",
    "            df (DataFrame): Data Frame with basic training data.\n",
    "        '''\n",
    "\n",
    "        df = pd.read_json(self.data_path, lines=self.read_lines)\n",
    "        logging.info(\"Data readed\")\n",
    "        df = self.filter_data(df)\n",
    "\n",
    "        return df\n",
    "    \n",
    "\n",
    "    def filter_data(self, data):\n",
    "        '''\n",
    "        Filters Data Frame for columns and string length.\n",
    "\n",
    "        Parameters:\n",
    "            data (DataFrame): Basic Trainings Data Frame.\n",
    "        \n",
    "        Returns:\n",
    "            filtered_data (DataFrame): Filtered trainings data.\n",
    "        '''\n",
    "\n",
    "        filtered_data = data[[\"paper_id\", \"source\", \"target\"]]\n",
    "        filtered_data = filtered_data.dropna(inplace=False)\n",
    "        if self.join_lists:\n",
    "            filtered_data[\"source\"] = filtered_data['source'].apply(lambda x: ' '.join(map(str, x)))\n",
    "            filtered_data[\"target\"] = filtered_data['target'].apply(lambda x: ' '.join(map(str, x)))\n",
    "        filtered_data = filtered_data[(filtered_data.source.astype(str).str.len()>self.source_min_max[0]) & (filtered_data.source.astype(str).str.len()<self.source_min_max[1])]\n",
    "        filtered_data = filtered_data[(filtered_data.target.astype(str).str.len()>self.target_min_max[0]) & (filtered_data.target.astype(str).str.len()<self.target_min_max[1])]\n",
    "        filtered_data[\"paper_id\"] = filtered_data[\"paper_id\"].astype(str)\n",
    "        \n",
    "        logging.info(\"Data filtered\")\n",
    "\n",
    "        return filtered_data\n",
    "    \n",
    "\n",
    "    def get_dict(self):\n",
    "        '''\n",
    "        Converts Training Data from Data Frame to Dictionary.\n",
    "        \n",
    "        Returns:\n",
    "            d (dict): Dictionary with trainings data.\n",
    "        '''\n",
    "\n",
    "        d = dict()\n",
    "        i = 1\n",
    "        for index, row in self.df.iterrows():\n",
    "            i = i + 1\n",
    "            d[row[\"paper_id\"]+\"_\"+str(i)] = [row[\"source\"], row[\"target\"]]\n",
    "        \n",
    "        logging.info(\"Dictionary created\")\n",
    "\n",
    "        return d\n",
    "\n",
    "\n",
    "    def get_train_test(self, split):   \n",
    "        '''\n",
    "        Splits trainings dictionary into trainings and test data.\n",
    "\n",
    "        Parameters:\n",
    "            split (int): Ratio for split between trainings and test data.\n",
    "        \n",
    "        Returns:\n",
    "            train_data (dict): Dictionary with data for training.\n",
    "            test_data (dict): Dictionary with data for testing.\n",
    "        '''\n",
    "\n",
    "        ids = list(self.dic.keys())\n",
    "        random.shuffle(ids)\n",
    "   \n",
    "        train_size = int(split * len(self.dic))\n",
    "\n",
    "        train_ids = ids[:train_size]\n",
    "        test_ids = ids[train_size:]\n",
    "\n",
    "        train_data = dict()\n",
    "        for id in train_ids:\n",
    "            train_data[id] = self.dic[id]\n",
    "\n",
    "        test_data = dict()\n",
    "        for id in test_ids:\n",
    "            test_data[id] = self.dic[id]\n",
    "        \n",
    "        logging.info(\"Data splitted into test and training\")\n",
    "\n",
    "        return train_data, test_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTDataset(Dataset):\n",
    "    '''\n",
    "    This is a class for encoding data to train nlp model.\n",
    "\n",
    "    Attributes:\n",
    "        data (dict): Dictionary of basic data.\n",
    "        tokenizer (tokenizer class): tokenizer for preparing inputs for model.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, data, tokenizer):\n",
    "        '''\n",
    "        Constructor for GPTDataset class.\n",
    "        \n",
    "        Parameters:\n",
    "            data (dict): Dictionary of basic data.\n",
    "            tokenizer (tokenizer class): tokenizer for preparing inputs for model.\n",
    "        '''\n",
    "\n",
    "        text, smry= [], []\n",
    "        for k, v in data.items():\n",
    "            text.append(v[0].strip().replace(\"\\n\", \" \"))\n",
    "            smry.append(v[1])\n",
    "\n",
    "        self.tokenizer = tokenizer \n",
    "        self.text      = text\n",
    "        self.smry      = smry\n",
    "\n",
    "        logging.info(\"GPTDataset instantiated\")\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        '''\n",
    "        Creates encoded input for model training.\n",
    "\n",
    "        Returns:\n",
    "            input_ids (torch.Tensor): Numerical representation of input tokens.\n",
    "            attention_mask (torch.Tensor): Information which tokens should be attended to. \n",
    "        '''\n",
    "        \n",
    "        input = SPECIAL_TOKENS[\"bos_token\"] + self.text[i] + \\\n",
    "            \"TL;DR:\" + self.smry[i] + SPECIAL_TOKENS[\"eos_token\"] \n",
    "\n",
    "        encodings_dict = tokenizer(input,                                   \n",
    "                                   truncation=True, \n",
    "                                   max_length=MAX_LENGTH, \n",
    "                                   padding=\"max_length\")   \n",
    "        \n",
    "        input_ids = encodings_dict[\"input_ids\"]\n",
    "        attention_mask = encodings_dict[\"attention_mask\"]\n",
    "        \n",
    "        logging.info(\"Encoded inputs created\")\n",
    "        \n",
    "        return {\"label\": torch.tensor(input_ids),\n",
    "                \"input_ids\": torch.tensor(input_ids), \n",
    "                \"attention_mask\": torch.tensor(attention_mask)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_tokenizer(MODEL, special_tokens=SPECIAL_TOKENS)\n",
    "model = get_model(MODEL, cuda_avbl, tokenizer, special_tokens=SPECIAL_TOKENS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "UNFREEZE_START = 12  \n",
    "UNFREEZE_STOP = 12                                                                                                  \n",
    "\n",
    "freeze_layers(model, UNFREEZE_START, UNFREEZE_STOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_data = BasicData(\"\", read_lines=True, join_lists=True, source_min_max=[10000, 70000], target_min_max=[800, 3000])\n",
    "\n",
    "train_data, val_data = example_data.get_train_test(TRAIN_SIZE)                                                        \n",
    "\n",
    "train_dataset = GPTDataset(train_data, tokenizer)                                                                       \n",
    "val_dataset = GPTDataset(val_data, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS          = 5\n",
    "TRAIN_BATCHSIZE = 1\n",
    "BATCH_UPDATE    = 1 \n",
    "STRATEGY        = \"epoch\"\n",
    "WS              = 1e2  #0\n",
    "LR              = 3e-4 #5e-5  \n",
    "EPS             = 1e-8\n",
    "WD              = 0.01\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"model_test\",\n",
    "    num_train_epochs=EPOCHS,                                                                                        # number of training epochs\n",
    "    per_device_train_batch_size=TRAIN_BATCHSIZE,                                                                    # batch size per GPU/CPU core for training\n",
    "    per_device_eval_batch_size=TRAIN_BATCHSIZE,                                                                     # batch size per GPU/CPU core for evaluation\n",
    "    gradient_accumulation_steps=BATCH_UPDATE,                                                                       # number of steps to accumulate the gradients\n",
    "    evaluation_strategy=STRATEGY,                                                                                   # when model should be evaluated\n",
    "    warmup_steps=WS,                                                                                                # steps from 0 to learing rate  \n",
    "    learning_rate=LR,                                                                                               # step size at each iteration\n",
    "    optim=\"adamw_torch\",                                                                                            # optimizer\n",
    "    adam_epsilon=EPS,                                                                                               # threshold for adaptive learning rates against zero division problems\n",
    "    weight_decay=WD,                                                                                                # regularization parameter to shrink model weights\n",
    "    disable_tqdm=False,                                                                                             # ensure the display of the progress bar while training\n",
    "    save_strategy=STRATEGY,                                                                                         # when model should be saved     \n",
    "    save_total_limit=1,                                                                                             # maximum number of saved models\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"     \n",
    ")\n",
    "\n",
    "\n",
    "# define trainer with model, arguments, data, tokenizer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,    \n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model()\n",
    "\n",
    "logging.info(\"Model trained and saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4 (tags/v3.9.4:1f2e308, Apr  6 2021, 13:40:21) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8cffa35e1638e1c79492d83b35465f30c828cbf3d6da9ee4a594aeef5efaf2bd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
